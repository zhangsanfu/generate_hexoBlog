---
title: Py007-02-03redis安装以及使用
date: 2018-12-01 21:54:22
tags: M07
---

### redis

```
自行去官网下载对应安装文件 我下的是5.0版本
https://redis.io/

http://www.redis.cn/
```

下载完成后解压缩后===> redis-5.0.0

```
该目录文件如下：
00-RELEASENOTES		Makefile		runtest-sentinel
BUGS			README.md		sentinel.conf
CONTRIBUTING		deps			src
COPYING			redis.conf		tests
INSTALL			runtest			utils

---------------------------------------------------
redis.conf这是一个配置文件（分布式爬虫时会要求对其进行配置）
src目录 这里是redis的源码文件 需要编译才能变成使用
```

> 安装redis

```
# 进入redis5.0.0 这个目录
# 执行
make 
# 成功之后再次进入 redis5.0.0/src
# 发现多了几个命令行文件
# 其中有俩重要的文件
redis-server 用来打开redis服务器
redis-cli 打开redis的客户端
```

> 启动redis服务器

- redis-server 通常结合一个配置(上一级目录的redis.conf)然后启动 

```
# 进入reids-server目录
cd redis5.0.0/src

./redis-server  ../redis.conf 
# 看到一个正方体 代表你启动成功
```

> 启动redis客户端

再开一个终端

```
# 进入reids-server目录
cd redis5.0.0/src

redis-cli

成功之后就可以进行相应的操作

如输入

> set name 'bobo'
> ok

> get name
> 'bobo'
```


#### 实战

> 依旧基于之前的糗百爬虫项目

修改 pipelines.py

```
# redis   安装 pip3 install redis
import redis

class QiubaiproPipeline(object):
    conn = None
    # 在爬虫过程中，该方法只会在开始爬虫的时候调用一次
    def open_spider(self,spider):
        print('开始爬虫')
        # 连接数据库
        self.conn = redis.Redis(host='127.0.0.1',port=6379)

    # 在爬虫过程中，该方法只会在爬虫结束的时候调用一次
    def close_spider(self, spider):
        print('结束爬虫')

    # 该方法接收 爬虫文件中提交过来的item对象，并对item对象中存储的数据进行持久化存储
    '''
    :param
    # 参数item:表示接收到的item对象
    '''
    def process_item(self, item, spider):
        # 取出item对应的数据值
        author = item['author']
        content = item['content']

        obj = {
            'author':author,
            'content':content
        }
        # 录入数据
        self.conn.lpush('data',obj)

        return item
```

> 启动redis服务

```
cd redis5.0.0/src

./redis-server   ../redis.conf
```

> 执行爬虫程序

```
scrapy crawl qiubai --nolog
```


> 启动redis客户端

```
cd redis5.0.0/src

./redis-cli
# 查看所有列表的数据
lrange data 0 -1
```