---
title: Py007-01-01爬虫初识
date: 2018-11-29 22:57:17
tags: M07
---

### python网络爬虫的简单介绍
 
> 什么是爬虫

爬虫就是通过编写程序模拟浏览器上网，然后让其去互联网上抓取数据的过程。

> 哪些语言可以实现爬虫
   
1. php：可以实现爬虫。php被号称是全世界最优美的语言（当然是其自己号称的，就是王婆卖瓜的意思），但是php在实现爬虫中支持多线程和多进程方面做的不好。

2. java：可以实现爬虫。java可以非常好的处理和实现爬虫，是唯一可以与python并驾齐驱且是python的头号劲敌。但是java实现爬虫代码较为臃肿，重构成本较大。

3. c、c++：可以实现爬虫。但是使用这种方式实现爬虫纯粹是是某些人（大佬们）能力的体现，却不是明智和合理的选择。

4. python：可以实现爬虫。python实现和处理爬虫语法简单，代码优美，支持的模块繁多，学习成本低，具有非常强大的框架（scrapy等）且一句难以言表的好！没有但是！

#### 爬虫的分类

> #### 1. 通用爬虫：通用爬虫是搜索引擎（Baidu、Google、Yahoo等）“抓取系统”的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。  简单来讲就是尽可能的；把互联网上的所有的网页下载下来，放到本地服务器里形成备分，在对这些网页做相关处理(提取关键字、去掉广告)，最后提供一个用户检索接口。 

``` 
搜索引擎如何抓取互联网上的网站数据？
门户网站主动向搜索引擎公司提供其网站的url
搜索引擎公司与DNS服务商合作，获取网站的url
门户网站主动挂靠在一些知名网站的友情链接中
```
> #### 2.聚焦爬虫：聚焦爬虫是根据指定的需求抓取网络上指定的数据。例如：获取豆瓣上电影的名称和影评，而不是获取整张页面中所有的数据值。

#### robots.txt协议

```
如果自己的门户网站中的指定页面中的数据不想让爬虫程序爬取到的话，那么则可以通过编写一个robots.txt的协议文件来约束爬虫程序的数据爬取。robots协议的编写格式可以观察淘宝网的robots（访问www.taobao.com/robots.txt即可）。但是需要注意的是，该协议只是相当于口头的协议，并没有使用相关技术进行强制管制，所以该协议是防君子不防小人。但是我们在学习爬虫阶段编写的爬虫程序可以先忽略robots协议。
```

#### 反爬虫

- 门户网站通过相应的策略和技术手段，防止爬虫程序进行网站数据的爬取。

#### 反反爬虫

- 爬虫程序通过相应的策略和技术手段，破解门户网站的反爬虫手段，从而爬取到相应的数据。
